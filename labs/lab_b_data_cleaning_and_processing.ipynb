{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "52e575b17eac4f55",
      "metadata": {
        "id": "52e575b17eac4f55"
      },
      "source": [
        "# M-Lab CARTE AI Workshop 2025 - Lab B: Data Cleaning and Processing\n",
        "\n",
        "In this lab, we’ll explore the foundational steps of **data cleaning and processing**, essential for preparing raw data for machine learning models. Using an energy forecasting dataset, we will work through common challenges in data preprocessing, including handling missing values, normalizing data, and addressing imbalanced classes.\n",
        "\n",
        "### Energy Market Forecasting Dataset\n",
        "\n",
        "The dataset used in this lab focuses on energy forecasting in Spain, combining information from multiple sources:\n",
        "- **Energy consumption and generation**: Data from ENTSOE (European Network of Transmission System Operators for Electricity).\n",
        "- **Settlement prices**: Market prices from Red Eléctrica España.\n",
        "- **Weather data**: Sourced from the Open Weather API for five major Spanish cities.\n",
        "\n",
        "This dataset was originally obtained from Kaggle, and can be found here: [Spanish High Resolution Energy Dataset](https://www.kaggle.com/nicholasjhana/energy-consumption-generation-prices-and-weather).\n",
        "\n",
        "This dataset is rich with **time series, numerical, and categorical data**, providing an excellent opportunity to learn practical preprocessing techniques. Forecasting energy demand and renewable energy generation is a real-world challenge with significant implications for transitioning to sustainable energy systems.\n",
        "\n",
        "### Key Steps in Lab\n",
        "\n",
        "1. **Data Inspection**:\n",
        "   - Explore the structure of the dataset using Pandas.\n",
        "   - Identify missing values and data inconsistencies.\n",
        "   - Generate a profiling report using `ydata-sdk` to visualize and understand the dataset with advanced data quality features.\n",
        "\n",
        "2. **Data Preprocessing**:\n",
        "   - **Normalization**: Scale numerical features to ensure equal weighting during model training.\n",
        "   - **One-Hot Encoding**: Convert categorical columns (e.g., city names) into a numeric format suitable for machine learning models.\n",
        "   - **Handling Missing Values**:\n",
        "     - Explore the trade-offs of removing rows or columns with missing values.\n",
        "     - Prepare a cleaned dataset by dropping irrelevant or incomplete columns.\n",
        "\n",
        "3. **Building a Baseline Model**:\n",
        "   - Train a **Random Forest Classifier** to predict weather conditions based on energy generation data.\n",
        "   - Evaluate the model’s accuracy and analyze its performance for each weather type.\n",
        "\n",
        "4. **Addressing Imbalanced Classes**:\n",
        "   - Group less common weather types into a single category to improve model performance.\n",
        "   - Re-train and evaluate the updated model.\n",
        "\n",
        "### Goals\n",
        "\n",
        "By the end of this lab, you will:\n",
        "- Understand key data preprocessing techniques and their importance for machine learning.\n",
        "- Learn to handle common challenges such as missing values, normalization, and imbalanced datasets.\n",
        "- Build and evaluate a simple machine learning model using Scikit-Learn.\n",
        "- Gain insight into the trade-offs and decisions involved in real-world data preparation.\n",
        "\n",
        "Let’s get started by loading the dataset and inspecting it to identify preprocessing needs!\n",
        "\n",
        "This line installs the **`ydata-sdk`** library, a tool for generating detailed reports about a dataset’s structure and contents. Let’s break it down:\n",
        "\n",
        "1. **The Exclamation Mark (`!`)**:\n",
        "   - In a Jupyter notebook or Google Colab, the exclamation mark allows you to run **shell commands** directly from a code cell. Here, `!pip install` is a shell command to install Python packages using `pip`, the Python package manager.\n",
        "   - Without the exclamation mark, the notebook would interpret this line as Python code, resulting in an error.\n",
        "\n",
        "2. **The Command Itself**:\n",
        "   - `pip install -Uq`:\n",
        "     - `-U`: Ensures the package is upgraded to the latest version.\n",
        "     - `-q`: Quiet mode, suppressing unnecessary output for a cleaner notebook.\n",
        "   - `ydata-sdk`:\n",
        "     - Installs the YData SDK which provides enhanced profiling capabilities including:\n",
        "       - **Data quality scoring**: Automated assessment of your data's quality\n",
        "       - **Redundancy detection**: Identifies duplicate and redundant information\n",
        "       - **Outlier identification**: Detects anomalies in your data\n",
        "       - **Text validation**: Validates text data patterns\n",
        "       - **Synthetic data generation**: Create realistic synthetic datasets\n",
        "\n",
        "3. **Why We’re Installing This Package**:\n",
        "   - Unlike common libraries like NumPy, Pandas, or Scikit-Learn (which are pre-installed in Google Colab), `ydata-sdk` is a specialized tool that isn’t included by default. Therefore, we need to explicitly install it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae460e587298af8e",
      "metadata": {
        "id": "ae460e587298af8e"
      },
      "outputs": [],
      "source": [
        "!pip install -Uq ydata-sdk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad29994d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enable custom widget manager for Google Colab (needed for ydata-profiling reports)\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    from google.colab import output\n",
        "    output.enable_custom_widget_manager()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d7418e677b44d78",
      "metadata": {
        "id": "5d7418e677b44d78"
      },
      "source": [
        "This block of code performs the following steps:\n",
        "\n",
        "1. **Importing Pandas**:\n",
        "   - `import pandas as pd`: Imports the **Pandas library**, a powerful tool for working with structured data like CSV files. It’s abbreviated as `pd` for convenience, which is the standard convention in Python.\n",
        "\n",
        "2. **Reading Compressed CSV Files**:\n",
        "   - `pd.read_csv()` is used to load data from CSV (comma-separated values) files. In this case, the files are hosted online and accessed via their URLs.\n",
        "\n",
        "   **Datasets Loaded**:\n",
        "   - `energy_dataset`: Contains data on electrical energy consumption, generation, and prices in Spain.\n",
        "   - `weather_features`: Contains weather data for the five largest cities in Spain.\n",
        "\n",
        "3. **Renaming Columns for Consistency**:\n",
        "   - `weather_features.rename(columns={'dt_iso': 'time'})`: Renames the `dt_iso` column in the `weather_features` DataFrame to `time`. This ensures consistency with the `energy_dataset`, where the corresponding column is named `time`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "id": "initial_id"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "energy_dataset = pd.read_csv('https://github.com/alexwolson/mdlw_materials/raw/refs/heads/main/data/energy_dataset.csv.gz')\n",
        "weather_features = pd.read_csv('https://github.com/alexwolson/mdlw_materials/raw/refs/heads/main/data/weather_features.csv.gz')\n",
        "\n",
        "# Rename `dt_iso` column to `time` for consistency\n",
        "weather_features = weather_features.rename(columns={'dt_iso': 'time'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc6a3f76a00f3ff",
      "metadata": {
        "id": "cc6a3f76a00f3ff"
      },
      "outputs": [],
      "source": [
        "# Look at some of the data\n",
        "energy_dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "985325085cf07f56",
      "metadata": {
        "id": "985325085cf07f56"
      },
      "outputs": [],
      "source": [
        "weather_features.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "945b71cc97e9519f",
      "metadata": {
        "id": "945b71cc97e9519f"
      },
      "source": [
        "**TODO: Answer basic questions related to datasets**\n",
        "\n",
        "Now that we’ve loaded the datasets, it’s time to inspect their structure and content. This step is crucial for understanding the available features, identifying potential issues, and planning the preprocessing steps.\n",
        "\n",
        "Try answering the following questions based on the datasets by running the cell below. \n",
        "\n",
        "1. **What cities are included in the weather data?**\n",
        "   - Inspect the `city_name` column in the `weather_features` DataFrame to find all unique city names.\n",
        "      - We use the `unique()` function to get a list of unique values in a column.\n",
        "\n",
        "2. **What are the columns in the energy dataset?**\n",
        "   - Review the column names in the `energy_dataset` DataFrame to understand the types of data available.\n",
        "      - We use the `columns` attribute to get a list of column names.\n",
        "\n",
        "3. **Do any of the datasets contain missing values?**\n",
        "   - Check for missing values in both datasets and determine which columns (if any) require special handling.\n",
        "      - We use the `isnull()` function to identify missing values, and `sum()` to count them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32e81c19fccae62f",
      "metadata": {
        "id": "32e81c19fccae62f"
      },
      "outputs": [],
      "source": [
        "print(f'Cities in the weather data: {weather_features.city_name.unique()}\\n')\n",
        "print(f'Columns in the energy dataset: {energy_dataset.columns}\\n')\n",
        "print(f'Missing values in the energy dataset:\\n{energy_dataset.isnull().sum()}\\n')\n",
        "print(f'Missing values in the weather data:\\n{weather_features.isnull().sum()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db2539ace3f31411",
      "metadata": {
        "id": "db2539ace3f31411"
      },
      "source": [
        "### Visualizing Data Distributions\n",
        "\n",
        "Visualizing columns in a dataset can help you understand the structure and patterns in the data. For example, by plotting the distribution of energy generation over time, you can identify trends, outliers, and other insights that inform your analysis.\n",
        "\n",
        "In Google Colab, you can visualize data quickly by printing columns or using a plotting library like **Matplotlib** or **Seaborn** for more advanced visualizations. Here, we’ll use Matplotlib to create a scatter plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2a9aff593087a50",
      "metadata": {
        "id": "b2a9aff593087a50"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert the 'time' column to datetime and extract the hour\n",
        "energy_dataset['time'] = pd.to_datetime(energy_dataset['time'], utc=True)\n",
        "energy_dataset['hour'] = energy_dataset['time'].dt.hour\n",
        "\n",
        "# Create a scatter plot\n",
        "energy_dataset.plot(x='hour', y='generation solar', kind='scatter')\n",
        "plt.title('Solar Generation by Hour')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bb4b609c94659ce",
      "metadata": {
        "id": "7bb4b609c94659ce"
      },
      "source": [
        "### Dataset Profiling with `ydata-sdk`\n",
        "\n",
        "The **`ydata-sdk`** library is a tool for generating comprehensive reports about the structure and content of a dataset. It automates much of the exploratory data analysis (EDA) process, providing insights that would otherwise require multiple lines of code to uncover.\n",
        "\n",
        "When applied to a dataset, `ydata-sdk` generates a report that includes:\n",
        "1. **Overview of the Dataset**:\n",
        "   - Number of variables (columns) and observations (rows).\n",
        "   - Data types of each column.\n",
        "2. **Missing Values**:\n",
        "   - Count and percentage of missing values in each column.\n",
        "3. **Distributions**:\n",
        "   - Histograms and statistics for numerical columns.\n",
        "   - Frequency counts for categorical columns.\n",
        "4. **Correlations**:\n",
        "   - Heatmaps and metrics to identify relationships between variables.\n",
        "5. **Potential Issues**:\n",
        "   - Constant or duplicate columns.\n",
        "   - Outliers or skewed distributions.\n",
        "\n",
        "_(Note: If the report doesn't seem to render properly in Google Colab, you can download the HTML file and open it in your browser.)_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ba69e2529f6e540",
      "metadata": {
        "id": "5ba69e2529f6e540"
      },
      "outputs": [],
      "source": [
        "from ydata_profiling import ProfileReport\n",
        "\n",
        "# Generate a profiling report for the energy dataset using ydata-sdk\n",
        "# This provides enhanced features including data quality scoring, redundancy detection,\n",
        "# outlier identification, and more advanced profiling capabilities\n",
        "profile = ProfileReport(energy_dataset, title='Energy Dataset Profiling Report', explorative=True)\n",
        "\n",
        "# Save the report as an HTML file\n",
        "profile.to_file('energy_dataset_profile.html')\n",
        "\n",
        "# Display the report in the notebook (if supported)\n",
        "profile.to_notebook_iframe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d9413f98e502d4a",
      "metadata": {
        "id": "5d9413f98e502d4a"
      },
      "outputs": [],
      "source": [
        "# Reload the initial data so we can do preprocessing\n",
        "energy_dataset = pd.read_csv('https://github.com/alexwolson/mdlw_materials/raw/refs/heads/main/data/energy_dataset.csv.gz')\n",
        "weather_features = pd.read_csv('https://github.com/alexwolson/mdlw_materials/raw/refs/heads/main/data/weather_features.csv.gz')\n",
        "weather_features = weather_features.rename(columns={'dt_iso': 'time'})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9907401953993f02",
      "metadata": {
        "id": "9907401953993f02"
      },
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "With a solid understanding of the dataset, the next step is to prepare the data for machine learning. **Preprocessing** ensures that the dataset is clean, consistent, and in a format suitable for training models. This process often includes:\n",
        "\n",
        "1. **Normalizing the Data**: Ensures numerical features are on a similar scale to prevent certain features from disproportionately influencing the model.\n",
        "2. **Converting Non-Numeric Data**: Transforms categorical or textual data into numeric representations that the model can process.\n",
        "3. **Handling Missing Values**: Ensures there are no gaps in the data, which could cause errors or inaccuracies in the model.\n",
        "4. **Splitting the Data**: Separates the data into features (inputs) and labels (outputs) for supervised learning tasks.\n",
        "\n",
        "### Why Normalize Data?\n",
        "\n",
        "Different features in a dataset may operate on vastly different ranges. For example:\n",
        "- One column might range from `0` to `1`.\n",
        "- Another column might range from `0` to `1000`.\n",
        "\n",
        "Without normalization, the model might treat the larger-range column as more important, simply because of its scale. By **normalizing** the data (subtracting the mean and dividing by the standard deviation), we:\n",
        "- Center each column around a mean of `0`.\n",
        "- Scale each column to have a standard deviation of `1`.\n",
        "\n",
        "This ensures that all features are treated equally by the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2d931cfd48f9ebd",
      "metadata": {
        "id": "e2d931cfd48f9ebd"
      },
      "outputs": [],
      "source": [
        "# Normalize numerical columns in the energy dataset\n",
        "for column in energy_dataset.columns:\n",
        "    if column != 'time' and energy_dataset[column].dtype != 'object':\n",
        "        energy_dataset[column] = (energy_dataset[column] - energy_dataset[column].mean()) / energy_dataset[column].std()\n",
        "\n",
        "# Normalize numerical columns in the weather features dataset\n",
        "for column in weather_features.columns:\n",
        "    if column != 'time' and weather_features[column].dtype != 'object':\n",
        "        weather_features[column] = (weather_features[column] - weather_features[column].mean()) / weather_features[column].std()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7e42b9090751217",
      "metadata": {
        "id": "a7e42b9090751217"
      },
      "outputs": [],
      "source": [
        "# Look at the normalized data\n",
        "energy_dataset.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c6dbb701e57e4ac",
      "metadata": {
        "id": "8c6dbb701e57e4ac"
      },
      "source": [
        "### Converting Text Data to Numeric: One-Hot Encoding\n",
        "\n",
        "Before we can move forward, we need to ensure that all the data is numeric, as most machine learning models require numerical input. However, some columns in our dataset, such as `city_name` and `weather_main`, contain text values. To address this, we will use **one-hot encoding**, a method of converting categorical data into a numeric format.\n",
        "\n",
        "### What is One-Hot Encoding?\n",
        "\n",
        "One-hot encoding transforms each unique value in a categorical column into a new binary column:\n",
        "- If a row contains a specific category, the corresponding binary column is set to `1`.\n",
        "- If not, it is set to `0`.\n",
        "\n",
        "For example, a `city_name` column with values `['Toronto', 'Montreal', 'Toronto']` would become:\n",
        "- `Toronto` column: `[1, 0, 1]`\n",
        "- `Montreal` column: `[0, 1, 0]`\n",
        "\n",
        "This approach ensures that all text data is represented numerically while preserving the original information.\n",
        "\n",
        "### Additional Cleanup\n",
        "\n",
        "After one-hot encoding, we will drop unnecessary columns, such as:\n",
        "- `weather_description`: A more detailed version of `weather_main` that is redundant for this task.\n",
        "- `weather_icon`: A visual representation not useful for numerical modeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0e484a0d1b00137",
      "metadata": {
        "id": "f0e484a0d1b00137"
      },
      "outputs": [],
      "source": [
        "# One-hot encode the 'city_name' and 'weather_main' columns\n",
        "weather_features = pd.get_dummies(weather_features, columns=['city_name', 'weather_main'])\n",
        "\n",
        "# Drop unnecessary columns\n",
        "weather_features = weather_features.drop(columns=['weather_description', 'weather_icon'])\n",
        "\n",
        "# Preview the modified dataset\n",
        "weather_features.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d6f6b1f905ce07a",
      "metadata": {
        "id": "5d6f6b1f905ce07a"
      },
      "source": [
        "### Handling Missing Values\n",
        "\n",
        "In real-world datasets, **missing values** are common and can disrupt machine learning models. To ensure data quality, it’s essential to handle them effectively. There are several strategies for managing missing values:\n",
        "\n",
        "1. **Removing Rows or Columns**: Eliminates data with missing values, but at the cost of potentially losing valuable information.\n",
        "2. **Imputing Missing Values**: Fills in missing data with an estimated value, such as the mean or median.\n",
        "3. **Using Models with Built-in Handling**: Some models can treat missing values as a separate category, preserving the dataset's structure.\n",
        "\n",
        "We’ll explore each method in turn and assess how they impact the performance of a machine learning model.\n",
        "\n",
        "---\n",
        "\n",
        "### Removing Rows with Missing Values\n",
        "\n",
        "The simplest way to handle missing values is to **remove any rows containing them**. While straightforward, this method can reduce the size of the dataset, possibly discarding useful data.\n",
        "\n",
        "In Pandas, the `dropna()` method makes it easy to remove rows with missing values. Let’s see how this affects the energy demand dataset by comparing the original dataset's shape to the modified version:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31c600b5bdd4ca5f",
      "metadata": {
        "id": "31c600b5bdd4ca5f"
      },
      "outputs": [],
      "source": [
        "# Remove rows with missing values\n",
        "energy_dataset_dropped = energy_dataset.dropna()\n",
        "\n",
        "# Compare the shape of the original and modified datasets\n",
        "print(f'Original data shape: {energy_dataset.shape}')\n",
        "print(f'Modified data shape: {energy_dataset_dropped.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30e84300d8de8789",
      "metadata": {
        "id": "30e84300d8de8789"
      },
      "source": [
        "Whoops! It looks like we have lost the significant majority of our dataset with this approach. If we inspect the data further, we can see which columns contain a significant number of missing values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb320a5215ea5fd8",
      "metadata": {
        "id": "cb320a5215ea5fd8"
      },
      "outputs": [],
      "source": [
        "# Count the number of missing values for each column\n",
        "missing_values = energy_dataset.isnull().sum()\n",
        "\n",
        "# Filter to show only columns with missing values\n",
        "missing_values = missing_values[missing_values > 0]\n",
        "\n",
        "# Display the result\n",
        "missing_values"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4368a6dd3f41037",
      "metadata": {
        "id": "d4368a6dd3f41037"
      },
      "source": [
        "As indicated earlier by the profiling report, we can see that there are a number of columns which don't contain any information. We can remove these columns and then try removing rows with missing values again:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7674d7282ab0981c",
      "metadata": {
        "id": "7674d7282ab0981c"
      },
      "outputs": [],
      "source": [
        "missing_columns = [\n",
        "    'generation fossil coal-derived gas',\n",
        "    'generation fossil oil shale',\n",
        "    'generation fossil peat',\n",
        "    'generation geothermal',\n",
        "    'generation hydro pumped storage aggregated',\n",
        "    'generation marine',\n",
        "    'generation wind offshore',\n",
        "    'forecast wind offshore eday ahead'\n",
        "]\n",
        "\n",
        "# Remove columns with a large number of missing values (and non-numeric columns)\n",
        "energy_dataset_removed = energy_dataset.drop(columns=missing_columns)\n",
        "\n",
        "# Remove rows with missing values\n",
        "energy_dataset_removed = energy_dataset_removed.dropna()\n",
        "\n",
        "# Look at the shape of the original and modified data\n",
        "print(f'Original data shape: {energy_dataset.shape}')\n",
        "print(f'Modified data shape: {energy_dataset_removed.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45db52ed14500b35",
      "metadata": {
        "id": "45db52ed14500b35"
      },
      "source": [
        "\n",
        "Now that we’ve removed rows with missing values, every row in our dataset has complete information. This makes the dataset easier to work with and ensures compatibility with machine learning models. However, this approach involves a tradeoff:\n",
        "- **Pros**: The dataset is clean and ready for modeling, with no missing values to handle.\n",
        "- **Cons**: We’ve reduced the dataset size, potentially losing valuable patterns and diversity in the data.\n",
        "\n",
        "There’s also a more subtle risk introduced by this method: **bias**. By removing rows with missing values, we might inadvertently remove certain types of data that are systematically incomplete, leading to a dataset that no longer accurately represents the real-world scenario. Can you think of situations where this could be an issue?\n",
        "\n",
        "### Building a Simple Machine Learning Model\n",
        "\n",
        "To explore the cleaned dataset, let’s build a simple machine learning model. Instead of predicting energy generation based on weather, we’ll flip the task:\n",
        "**Predict the current weather type based on the current energy generation.**\n",
        "\n",
        "This reverse prediction might seem unexpected, but it’s a useful exercise to understand the relationship between features (energy generation) and labels (weather type).\n",
        "\n",
        "We will use a **Random Forest Classifier**, a versatile and robust machine learning algorithm that:\n",
        "- Combines multiple decision trees to improve accuracy (an _ensemble_ method).\n",
        "- Handles both classification and regression tasks well.\n",
        "\n",
        "### Splitting Data into Features and Labels\n",
        "Before building the model, we need to:\n",
        "1. **Define Features**: These are the input variables (energy generation data) used for prediction.\n",
        "2. **Define Labels**: These are the target variables (weather type) we aim to predict."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24554553fea40c92",
      "metadata": {
        "id": "24554553fea40c92"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from time import time\n",
        "\n",
        "# Merge energy demand and weather datasets on the 'time' column using an inner join\n",
        "# This ensures only rows with matching time entries in both datasets are included\n",
        "# Convert 'time' columns to datetime format to ensure they can be merged\n",
        "# This handles the case where one might be a string and the other a datetime\n",
        "energy_dataset_removed['time'] = pd.to_datetime(energy_dataset_removed['time'], utc=True)\n",
        "weather_features['time'] = pd.to_datetime(weather_features['time'], utc=True)\n",
        "\n",
        "df = pd.merge(energy_dataset_removed, weather_features, on='time', how='inner')\n",
        "\n",
        "# Select features for the model (energy generation data)\n",
        "# These are the columns that will be used to predict the weather type\n",
        "X = df[['generation biomass',\n",
        "        'generation fossil brown coal/lignite',\n",
        "        'generation fossil gas',\n",
        "        'generation fossil hard coal',\n",
        "        'generation fossil oil',\n",
        "        'generation hydro pumped storage consumption',\n",
        "        'generation hydro run-of-river and poundage',\n",
        "        'generation hydro water reservoir',\n",
        "        'generation nuclear',\n",
        "        'generation other',\n",
        "        'generation other renewable',\n",
        "        'generation solar',\n",
        "        'generation waste',\n",
        "        'generation wind onshore']]\n",
        "\n",
        "# Select labels for the model (one-hot encoded weather types)\n",
        "# Columns starting with 'weather_main' represent the one-hot encoded target classes\n",
        "y = df[[column for column in df.columns if column.startswith('weather_main')]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "870f792835b40f1e",
      "metadata": {
        "id": "870f792835b40f1e"
      },
      "outputs": [],
      "source": [
        "# Split the dataset into training and testing sets\n",
        "# 80% of the data is used for training, and 20% for testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# Reset the indices for the training and testing sets\n",
        "# This is helpful to ensure the indices are sequential and clean after the split\n",
        "X_train = X_train.reset_index(drop=True)\n",
        "X_test = X_test.reset_index(drop=True)\n",
        "y_train = y_train.reset_index(drop=True)\n",
        "y_test = y_test.reset_index(drop=True)\n",
        "\n",
        "# Print the shapes of the training and testing datasets\n",
        "print(f'X_train shape: {X_train.shape}')\n",
        "print(f'X_test shape: {X_test.shape}')\n",
        "print(f'y_train shape: {y_train.shape}')\n",
        "print(f'y_test shape: {y_test.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8817efef5f5a947f",
      "metadata": {
        "id": "8817efef5f5a947f"
      },
      "outputs": [],
      "source": [
        "# Initialize the Random Forest model\n",
        "# n_jobs=-1 allows the model to use all available CPU cores for faster training\n",
        "model = RandomForestClassifier(n_jobs=-1)\n",
        "\n",
        "# Fit the model to the training data\n",
        "print('Fitting model...')\n",
        "start_time = time()  # Track the start time for training\n",
        "model.fit(X_train, y_train)\n",
        "print(f'Model fit in {time() - start_time:.0f} seconds')  # Print training time\n",
        "\n",
        "# Evaluate the model on the testing data\n",
        "print('Evaluating model...')\n",
        "predictions = model.predict(X_test)  # Predict weather types for the test set\n",
        "accuracy = accuracy_score(y_test, predictions)  # Calculate the accuracy of the predictions\n",
        "\n",
        "# Print the model's accuracy\n",
        "print(f'Model accuracy: {accuracy*100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "505186a448304567",
      "metadata": {
        "id": "505186a448304567"
      },
      "source": [
        "After training the Random Forest model, you might observe an accuracy of approximately **44%**. This result is promising, especially given that we are predicting one of **12 different weather types**. If predictions were made randomly, the expected accuracy would be around **8%**, so this is a significant improvement.\n",
        "\n",
        "However, accuracy alone doesn't tell the full story. The model may perform better for certain weather types than others, especially if some types are more common in the dataset. To understand this, we can calculate the accuracy for each weather type individually.\n",
        "\n",
        "### Evaluating Per-Weather-Type Accuracy\n",
        "\n",
        "To break down the accuracy by weather type:\n",
        "1. **Count**: Determine how many test samples belong to each weather type.\n",
        "2. **Correct**: Count how many predictions for that type are correct.\n",
        "3. **Accuracy**: Compute the proportion of correct predictions for each type.\n",
        "\n",
        "The code below prints a table showing the performance for each weather type:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad3535a5c41dd327",
      "metadata": {
        "id": "ad3535a5c41dd327"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "print(\"Weather Type              | Count | Correct | Accuracy\")\n",
        "print(\"--------------------------|-------|---------|---------\")\n",
        "\n",
        "for i, column in enumerate(y_test.columns):\n",
        "    count = y_test[column].sum()  # Total number of samples for this weather type\n",
        "    indices = y_test[y_test[column]].index  # Indices of samples for this type\n",
        "    predictions_for_column = predictions[indices]  # Predictions for this type\n",
        "    correct = predictions_for_column[:, i].sum()  # Correct predictions for this type\n",
        "    accuracy = correct / count if count > 0 else np.nan  # Avoid division by zero\n",
        "\n",
        "    print(f'{column:<25} | {count:<5} | {correct:<7} | {accuracy:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "759c532d03c1bdb8",
      "metadata": {
        "id": "759c532d03c1bdb8"
      },
      "source": [
        "Our evaluation has uncovered a major flaw in the model: it performs well for common weather types like `clear`, `clouds`, and `rain`, but struggles with the remaining weather conditions. This is largely due to an **imbalance** in the dataset, where most examples fall into just a few categories, leaving the other weather types underrepresented.\n",
        "\n",
        "### Why is this a problem?\n",
        "\n",
        "- The model learns patterns based on the data it sees most frequently, so it becomes biased toward the majority classes.\n",
        "- Rare weather conditions have fewer examples, making it harder for the model to recognize them reliably.\n",
        "- This results in poor generalization for less common weather types, even if they are critical for certain applications.\n",
        "\n",
        "### Solution: Group Rare Weather Types\n",
        "\n",
        "To address this imbalance, we will:\n",
        "1. Retain the original categories for `clear`, `clouds`, and `rain`.\n",
        "2. Combine all other weather types into a single category called `weather_main_other`.\n",
        "\n",
        "This reduces the total number of categories and provides the model with more examples for the combined \"other\" category, improving its ability to generalize.\n",
        "\n",
        "### Code to Group Weather Types\n",
        "\n",
        "The following code creates the new `weather_main_other` category:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44fe028a19d0be4b",
      "metadata": {
        "id": "44fe028a19d0be4b"
      },
      "outputs": [],
      "source": [
        "# Retain columns for clear, clouds, and rain\n",
        "y_grouped = y[['weather_main_clear', 'weather_main_clouds', 'weather_main_rain']].copy()\n",
        "\n",
        "# Add a new column for \"other\" weather conditions\n",
        "y_grouped['weather_main_other'] = ~y_grouped.any(axis=1)  # True if none of the main categories apply\n",
        "\n",
        "# Summarize the counts for each category\n",
        "y_grouped.sum(axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb6f1c675b38da7",
      "metadata": {
        "id": "cb6f1c675b38da7"
      },
      "source": [
        "After grouping the less common weather types into a single category (`weather_main_other`), our dataset is now more balanced, with sufficient examples for each category. This should allow the model to better generalize across all weather types. Let’s retrain the Random Forest model and evaluate its performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a29e113d6f4e772",
      "metadata": {
        "id": "a29e113d6f4e772"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_grouped, test_size=0.2, random_state=42)\n",
        "\n",
        "# Reset indices for clean data\n",
        "X_train = X_train.reset_index(drop=True)\n",
        "X_test = X_test.reset_index(drop=True)\n",
        "y_train = y_train.reset_index(drop=True)\n",
        "y_test = y_test.reset_index(drop=True)\n",
        "\n",
        "# Train the Random Forest model\n",
        "model = RandomForestClassifier()\n",
        "print('Fitting model...')\n",
        "start_time = time()\n",
        "model.fit(X_train, y_train)\n",
        "print(f'Model fit in {time() - start_time:.0f} seconds')\n",
        "\n",
        "# Evaluate the model\n",
        "print('Evaluating model...')\n",
        "predictions = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f'Model accuracy: {accuracy * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89eef7e8840f75b4",
      "metadata": {
        "id": "89eef7e8840f75b4"
      },
      "outputs": [],
      "source": [
        "print(\"Column                    | Count | Correct | Accuracy\")\n",
        "print(\"--------------------------|-------|---------|---------\")\n",
        "for i, column in enumerate(y_test.columns):\n",
        "    count = y_test[column].sum()\n",
        "    indices = y_test[y_test[column]].index\n",
        "    predictions_for_column = predictions[indices]\n",
        "    correct = predictions_for_column[:, i].sum()\n",
        "    accuracy = correct / count if count > 0 else np.nan\n",
        "\n",
        "    print(f'{column:<25} | {count:<5} | {correct:<7} | {accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2968ac99926e25de",
      "metadata": {
        "id": "2968ac99926e25de"
      },
      "source": [
        "### Challenges with Imbalanced Classes\n",
        "\n",
        "Despite our efforts to balance the dataset by grouping less common weather types, the model still struggles to accurately predict these categories. This is a common issue in machine learning, where models tend to be **biased toward the most common classes**.\n",
        "\n",
        "This bias occurs because:\n",
        "- The model has more examples to learn from for the frequent classes.\n",
        "- Rare classes contribute less to the overall loss during training, leading the model to optimize for the majority classes.\n",
        "\n",
        "### Strategies to Address Class Imbalance\n",
        "There are several ways to tackle this challenge:\n",
        "\n",
        "1. **Collect More Data for Less Common Classes**:\n",
        "   - Increase the representation of rare weather types by collecting additional samples.\n",
        "   - This improves the model’s ability to learn patterns for these classes.\n",
        "\n",
        "2. **Use a Model That Handles Imbalanced Classes Better**:\n",
        "   - Some models (e.g., Gradient Boosting, specialized neural networks) are better suited for imbalanced datasets.\n",
        "   - Techniques like class weighting can also help emphasize the importance of rare classes during training.\n",
        "\n",
        "3. **Oversampling or Undersampling**:\n",
        "   - **Oversampling**: Duplicate or generate synthetic data for the less common classes to increase their representation.\n",
        "   - **Undersampling**: Reduce the number of samples from the majority classes to create a more balanced dataset.\n",
        "\n",
        "### What’s Next?\n",
        "We will explore some of these approaches in more detail later in the workshop. For now, keep in mind that handling imbalanced datasets is a crucial step in building fair and effective machine learning models. It often requires a combination of data collection, preprocessing, and model tuning to achieve the best results.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "In this exercise, we explored the challenges of building a machine learning model to predict weather types based on energy generation data. Along the way, we learned about key steps in the machine learning workflow, including:\n",
        "\n",
        "- **Handling Missing Data**: Removing rows with missing values and discussing the tradeoffs.\n",
        "- **Balancing Classes**: Grouping rare weather types into a single category to create a more balanced dataset.\n",
        "- **Training and Evaluating Models**: Using a Random Forest Classifier to make predictions and assess its performance.\n",
        "\n",
        "While our efforts improved the dataset’s balance, the model still struggled with the less common weather types. This highlights an important lesson: addressing class imbalance often requires a combination of advanced techniques and additional data collection.\n",
        "\n",
        "### Key Takeaways:\n",
        "1. Real-world datasets often contain imbalances and missing values, which require thoughtful preprocessing.\n",
        "2. Even with preprocessing, model performance can remain biased toward majority classes, necessitating additional strategies like oversampling or class-weighted models.\n",
        "3. Machine learning is an iterative process, requiring experimentation and refinement to achieve better results.\n",
        "\n",
        "---\n",
        "\n",
        "### Bonus Task: Using Oversampling to Improve Performance\n",
        "\n",
        "If you’ve finished early and want to dig deeper, try using **oversampling** to improve the model’s ability to predict less common weather types. Oversampling involves increasing the representation of the minority classes in the training dataset. This can be done using libraries like `imbalanced-learn`, which provides tools to handle imbalanced datasets effectively.\n",
        "\n",
        "#### Steps to Implement Oversampling:\n",
        "1. **Install imbalanced-learn**:\n",
        "   - If not already installed, use `pip install imbalanced-learn` to add this library to your environment.\n",
        "\n",
        "2. **Apply SMOTE (Synthetic Minority Oversampling Technique)**:\n",
        "   - Use `SMOTE` from `imbalanced-learn` to generate synthetic samples for the minority classes in the training data.\n",
        "   - SMOTE creates new samples by interpolating between existing samples, which avoids duplication and makes the training data more diverse.\n",
        "\n",
        "3. **Integrate Oversampling into the Pipeline**:\n",
        "   - First, split your dataset into training and testing sets as before.\n",
        "   - Apply SMOTE to the training set only, ensuring the test set remains representative of real-world data.\n",
        "\n",
        "4. **Train and Evaluate the Model**:\n",
        "   - Train your Random Forest model on the oversampled training set.\n",
        "   - Evaluate its performance on the original test set and compare the results to your previous model.\n",
        "\n",
        "#### Example Methods:\n",
        "- Use `from imblearn.over_sampling import SMOTE` to import the SMOTE class.\n",
        "- Apply `SMOTE().fit_resample(X_train, y_train)` to oversample the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3957291a8fd16593",
      "metadata": {
        "id": "3957291a8fd16593"
      },
      "outputs": [],
      "source": [
        "!pip install imbalanced-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a08a0357e684f84",
      "metadata": {
        "id": "7a08a0357e684f84"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Apply SMOTE to oversample the minority classes\n",
        "smote = SMOTE()\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train.values, y_train.values)\n",
        "\n",
        "# Train the Random Forest model on the resampled data\n",
        "model_resampled = RandomForestClassifier(n_jobs=-1)\n",
        "print('Fitting model...')\n",
        "start_time = time()\n",
        "model_resampled.fit(X_train_resampled, y_train_resampled)\n",
        "print(f'Model fit in {time() - start_time:.0f} seconds')\n",
        "\n",
        "# Evaluate the model on the original test set\n",
        "print('Evaluating model...')\n",
        "predictions_resampled = model_resampled.predict(X_test)\n",
        "accuracy_resampled = accuracy_score(y_test, predictions_resampled)\n",
        "print(f'Model accuracy with oversampling: {accuracy_resampled * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f50d0cb6a572b8c8",
      "metadata": {
        "id": "f50d0cb6a572b8c8"
      },
      "outputs": [],
      "source": [
        "print(\"Column                    | Count | Correct | Accuracy\")\n",
        "print(\"--------------------------|-------|---------|---------\")\n",
        "for i, column in enumerate(y_test.columns):\n",
        "    count = y_test[column].sum()\n",
        "    indices = y_test[y_test[column]].index\n",
        "    predictions_for_column = predictions_resampled[indices]\n",
        "    correct = predictions_for_column[:, i].sum()\n",
        "    accuracy = correct / count if count > 0 else np.nan\n",
        "\n",
        "    print(f'{column:<25} | {count:<5} | {correct:<7} | {accuracy:.4f}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "mlab",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
